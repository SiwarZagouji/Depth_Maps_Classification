{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44a95da",
   "metadata": {},
   "source": [
    "## <center> MLP Classifier applied on RGBD images  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37346075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "166e6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total indoor images: 829\n",
      "Total indoor depth maps: 829\n",
      "Total outdoor images: 834\n",
      "Total outdoor depth maps: 834\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'C:\\Users\\Siwar\\Downloads\\data'  \n",
    "indoor_dir = os.path.join(data_dir, 'indoors')  # Path to the indoor images and depth maps folder\n",
    "outdoor_dir = os.path.join(data_dir, 'outdoors')  # Path to the outdoor images and depth maps folder\n",
    "\n",
    "# Function to read and process the images and depth maps\n",
    "def read_data(directory):\n",
    "    images = []\n",
    "    depth_maps = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith('.png'):\n",
    "            img = Image.open(file_path)  \n",
    "            images.append(img)\n",
    "        elif filename.endswith('.npy'):\n",
    "            depth_map = np.load(file_path)  \n",
    "            depth_maps.append(depth_map)\n",
    "    return images, depth_maps\n",
    "\n",
    "# Read indoor images and indoor depth maps\n",
    "indoor_images, indoor_depth_maps = read_data(indoor_dir)\n",
    "print(f\"Total indoor images: {len(indoor_images)}\")\n",
    "print(f\"Total indoor depth maps: {len(indoor_depth_maps)}\")\n",
    "\n",
    "# Read outdoor images and outdoor depth maps\n",
    "outdoor_images, outdoor_depth_maps = read_data(outdoor_dir)\n",
    "print(f\"Total outdoor images: {len(outdoor_images)}\")\n",
    "print(f\"Total outdoor depth maps: {len(outdoor_depth_maps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874e6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to divide an image into smaller sub-images\n",
    "def divide_image(image):\n",
    "    sub_images = [image]\n",
    "    for _ in range(2):\n",
    "        new_sub_images = []\n",
    "        for sub_image in sub_images:\n",
    "            width, height = sub_image.size\n",
    "            sub_width = width // 2\n",
    "            sub_height = height // 2\n",
    "            top_left = sub_image.crop((0, 0, sub_width, sub_height))\n",
    "            top_right = sub_image.crop((sub_width, 0, width, sub_height))\n",
    "            bottom_left = sub_image.crop((0, sub_height, sub_width, height))\n",
    "            bottom_right = sub_image.crop((sub_width, sub_height, width, height))\n",
    "            new_sub_images.extend([top_left, top_right, bottom_left, bottom_right])\n",
    "        sub_images = new_sub_images\n",
    "    return sub_images\n",
    "\n",
    "#Function to calculate the average of an RGB sub-image\n",
    "def calculate_average_rgb(sub_images):\n",
    "    averages = []\n",
    "    for sub_image in sub_images:\n",
    "        rgb_average = np.array(sub_image).mean(axis=(0, 1))\n",
    "        averages.append(rgb_average)\n",
    "    return np.array(averages)\n",
    "\n",
    "#Function to calculate the average of a depth map sub-image\n",
    "def calculate_average_depth(depth_map):\n",
    "    sub_height, sub_width = depth_map.shape[0] // 4, depth_map.shape[1] // 4\n",
    "    average_depth_values = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            sub_image = depth_map[i * sub_height:(i + 1) * sub_height, j * sub_width:(j + 1) * sub_width]\n",
    "            average_depth = np.mean(sub_image)\n",
    "            average_depth_values.append(average_depth)\n",
    "    return np.array(average_depth_values)\n",
    "\n",
    "# Function to create an RGBD tensor from RGB image and its corresponding depth map\n",
    "def create_rgbd_tensor(rgb_image_path, depth_map_path):\n",
    "    rgb_image = Image.open(rgb_image_path)\n",
    "    sub_images = divide_image_recursive(rgb_image)\n",
    "    average_rgb_values = calculate_average_rgb(sub_images)\n",
    "    depth_map = np.load(depth_map_path, allow_pickle=True)\n",
    "    average_depth_values = calculate_average_depth(depth_map)\n",
    "    matrix = np.column_stack((average_rgb_values, average_depth_values))\n",
    "    rgbd_tensor = torch.tensor(matrix)\n",
    "    return rgbd_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e66e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of indoor RGBD tensors\n",
    "indoor_tensors = []\n",
    "for filename in os.listdir(indoor_dir):\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(indoor_dir, filename)\n",
    "        depth_map_filename = filename.split('.')[0] + '_depth.npy'\n",
    "        depth_map_path = os.path.join(indoor_dir, depth_map_filename)\n",
    "        image = Image.open(image_path)\n",
    "        sub_images = divide_image(image)\n",
    "        average_rgb_values = calculate_average_rgb(sub_images)\n",
    "        depth_map = np.load(depth_map_path)\n",
    "        average_depth_values = calculate_average_depth(depth_map)\n",
    "        matrix = np.column_stack((average_rgb_values, average_depth_values))\n",
    "        tensor = torch.tensor(matrix)\n",
    "        indoor_tensors.append(torch.flatten(tensor))\n",
    "\n",
    "# Creating a list of outdoor RGBD tensors\n",
    "outdoor_tensors = []\n",
    "for filename in os.listdir(outdoor_dir):\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(outdoor_dir, filename)\n",
    "        depth_map_filename = filename.split('.')[0] + '_depth.npy'\n",
    "        depth_map_path = os.path.join(outdoor_dir, depth_map_filename)\n",
    "        image = Image.open(image_path)\n",
    "        sub_images = divide_image(image)\n",
    "        average_rgb_values = calculate_average_rgb(sub_images)\n",
    "        depth_map = np.load(depth_map_path)\n",
    "        average_depth_values = calculate_average_depth(depth_map)\n",
    "        matrix = np.column_stack((average_rgb_values, average_depth_values))\n",
    "        tensor = torch.tensor(matrix)\n",
    "        outdoor_tensors.append(torch.flatten(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d07644e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of every indoor or outdoor tensor\n",
    "indoor_tensors[0].shape, outdoor_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1b6b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indoor tensors - Train: 663\n",
      "Indoor tensors - Test: 166\n",
      "Outdoor tensors - Train: 667\n",
      "Outdoor tensors - Test: 167\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "indoor_tensors_train, indoor_tensors_test = train_test_split(indoor_tensors, test_size=0.2, random_state=42)\n",
    "outdoor_tensors_train, outdoor_tensors_test = train_test_split(outdoor_tensors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Indoor tensors - Train:\", len(indoor_tensors_train))\n",
    "print(\"Indoor tensors - Test:\", len(indoor_tensors_test))\n",
    "print(\"Outdoor tensors - Train:\", len(outdoor_tensors_train))\n",
    "print(\"Outdoor tensors - Test:\", len(outdoor_tensors_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c29554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data and labels\n",
    "indoor_train_data = torch.stack(indoor_tensors_train)\n",
    "outdoor_train_data = torch.stack(outdoor_tensors_train)\n",
    "train_data = torch.cat([indoor_train_data, outdoor_train_data])\n",
    "train_labels = [0] * len(indoor_tensors_train) + [1] * len(outdoor_tensors_train)\n",
    "\n",
    "#test data and labels\n",
    "indoor_test_data = torch.stack(indoor_tensors_test)\n",
    "outdoor_test_data = torch.stack(outdoor_tensors_test)\n",
    "test_data = torch.cat([indoor_test_data, outdoor_test_data])\n",
    "test_labels = [0] * len(indoor_tensors_test) + [1] * len(outdoor_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70a74493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 333)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d5ddde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.001, batch_size=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, batch_size=200, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.001, batch_size=200, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mlp classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), batch_size=200, solver='adam', learning_rate_init=0.001, activation='relu', alpha= 0.001,  random_state=42)\n",
    "mlp.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a7c2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244d136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Recall: 0.9\n",
      "Precision: 0.75\n",
      "F1 Score: 0.82\n",
      "AUC-ROC: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Accuracy:\", round(accuracy,2))\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(test_labels, predictions)\n",
    "print(\"Recall:\", round(recall,2))\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(test_labels, predictions)\n",
    "print(\"Precision:\", round(precision,2))\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "print(\"F1 Score:\", round(f1,2))\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc = roc_auc_score(test_labels, predictions)\n",
    "print(\"AUC-ROC:\", round(auc,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
